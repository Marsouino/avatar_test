# Infrastructure Hardware — LIRE config/hardware.yaml

## Règle principale

**AVANT toute installation de package ML, commande CUDA, ou travail infra :**
→ Lire `config/hardware.yaml` pour les versions exactes et contraintes de l'environnement cible.

## Rappels critiques

1. **Versions PyTorch/CUDA** — Ne jamais deviner. Toujours vérifier `config/hardware.yaml`.
2. **Environnements multiples** — Local et production peuvent avoir des versions différentes. Vérifier lequel est ciblé.
3. **Containers** — Si l'environnement utilise des containers NVIDIA, NE PAS installer PyTorch/TensorFlow sur l'host.
4. **Réseau/Storage** — Les paths et IPs sont dans `config/hardware.yaml`, section `network` / `storage`.

## Debugging GPU

```python
import torch
print(f"PyTorch: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"CUDA version: {torch.version.cuda}")
print(f"GPU: {torch.cuda.get_device_name(0)}")
print(f"Compute capability: {torch.cuda.get_device_capability(0)}")
```

Comparer les résultats avec les valeurs dans `config/hardware.yaml`.
